{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5KPh1NtCCDja7gCIiI9dZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicklausmillican/StatisticalRethinkingIISolutions/blob/main/StatisticalRethinkingSolutions2_Ch7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7"
      ],
      "metadata": {
        "id": "TX44yv15HT9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JPYvU_ERsQ-"
      },
      "outputs": [],
      "source": [
        "install.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\"))\n",
        "devtools::install_github(\"rmcelreath/rethinking@slim\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(rethinking)"
      ],
      "metadata": {
        "id": "FFONo5eeR7ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Easy"
      ],
      "metadata": {
        "id": "pJvvqpdCSIOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7E1\n",
        "#### Question\n",
        "State the three motivating criteria that define information entropy. Try to express each in your  own words.\n",
        "\n",
        "#### Answer\n",
        "Entropy is a measure of uncertainty; information resolves that uncertainty (e.g., with evidence).  So entropy *feels* a bit like probability in that both deal with levels of (un)certainty.  It shouldn't be surprising, then, that entropy H is a function of probability.\n",
        "\n",
        "$$H(X) = \\sum_{i=1}^n p(X=x_i) \\times log_b(\\frac{1}{p(X=x_i)})$$\n",
        "$$= - \\sum_{i=1}^n p(X=x_i) \\times log_b(p(X=x_i))$$\n",
        "$$= -E[log_b(p(X=x_i))]$$\n",
        "\n",
        "How to interpret?  Think of $log_b(\\frac{1}{p(X=x_i)})$ as \"surprise\": the greater the probability $p(X=x_i)$, the less surprising it is when that event occurs; conversely, the less probable an event $p(X=x_i)$, the more surprising it is when it occurs.  Next, we *weight* the probability of each event $p(X=x_i)$ by its probability of occuring $p(X=x_i)$ and sum over each event $X=x_i$.  This gives us a *weighted average* of surprise for $X$.  We actually use the $log$ of $\\frac{1}{p(X=x_i)}$ in order to achieve a few desirable traits for our measure of uncertainty.\n",
        "\n",
        "From p. 205:\n",
        "1.   **Continuity:** Just as we want our (un)certainty to be able to slide smoothly from completely uncertain to completely certain, we want informational entropy to do the same.  Probability accomplishes this and, as a function of probability, the formula for $H$ permits the same.\n",
        "2.   **Proportionality:** All else being equal, more potential outcomes for $X$ should increase our uncertainty about $X$.  You can see how the formula for $H$ accomplishes this.  If instead of being, say, 3 values for $X$ there are 4, $H$ becomes the sum of 4 terms instead of only 3; plus, the additional 4th value for $X$ means that the probability of each possible value has probably decreased--thus increasing the surprise for each.  All told, $H$ increases.\n",
        "3.   **Additivity:** If we have two events about which we are uncertain, it is desirable to say that our total uncertainty is the sum of those events.  Similarly for more events.  The formula for $H$ accomplishes this by the $\\sum$ operator.  We sum the surprise, weighted by its probability of happening, of each possible event.\n",
        "\n",
        "Additionally, there are some other important features of information entropy no mentioned in the book:\n",
        "4.   **Non-Negativity:** We cannot be negatively uncertain about an event; we can only be completely certain--which is 0 uncertainty.  The formula for $H$ accomplishes this by its use of probabilities: probabilities also cannot be negative.\n",
        "5.   **Maximal Value:** Just as there is a minimal entropy (0), there is a maximal entropy.  Maximal entropy occurs when each possible value are equally likely.  You can see this in the formula for $H$: if the probability of any event becomes greater than $\\frac{1}{n}$, then the probabilities of other events must decrease...increasing their surprise.  This results in less overall entropy.\n",
        "\n",
        "> This chapter addresses a concept called \"MaxEnt\" or \"maximal entropy\".  This is a similar concept, but it refers to the maximal entropy of a variable under some set of contraints.\n",
        "\n",
        "There are many other features of informational entropy that we could list.  But we'll stop here.\n",
        "\n",
        "If you're interested in a deeper but still-understandable resource on information theory, I suggest [Probability and Information: An Integrated Approach 2nd Edition](https://www.amazon.com/Probability-Information-Integrated-David-Applebaum/dp/0521899044) and [Information Theory: A Tutorial Introduction (2nd Edition)](https://www.amazon.com/Information-Theory-Tutorial-Introduction-2nd/dp/1739672704/ref=sr_1_1?crid=QB4WC07L2QV2&dib=eyJ2IjoiMSJ9.HdyNIMnteFZLf7Ghuh6b4KpfMMBis3Cg2Cn4pOhcL08uhNjOVjY5qqMtASytMwiCDZNo8atQ_BvoUOLSeLvzJMSkRrLHGJNdYa3VnLzWgodcgfMRGbJkHt5VFKslyyzX4JYNra34ExCHrvPo7sXCCkIN3NFpJom82G6K_FzCkaU-mOKz-PkQ3CnNhjkBzmYSdIkVdNUmWSE-Di1EjxG_4rWfCra_68Z8zvOI4yM1ub0uKR_QGV0xFv66L61PHlPS25GTH9hCAOTv5q0nezWOtHUX9fO6YEyj17fuskNrV5M.dpOwik_DNtNOYUJyMa8EC825qBdeL0uQJYgazlfU7l0&dib_tag=se&keywords=information+theory&qid=1709823456&s=books&sprefix=information+theory%2Cstripbooks%2C151&sr=1-1-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)."
      ],
      "metadata": {
        "id": "nm9yt5lBSKk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7E2-7E4\n",
        "#### Questions\n",
        "2.   Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?\n",
        "3.   Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2”  25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?  \n",
        "4.   Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?\n",
        "\n",
        "#### Answers\n",
        "We'll group these answers, since they all follow the same process.  We just need to define the probabilities given in the question, then implement our formula for entropy.\n",
        "\n",
        "For each, I'll calculate entropy in a few different ways, and also compare it to the maximal entropy for that situation described in the question.\n",
        "\n",
        "Finally, I simulate data according to the situation described in the problem, and then calculate the empirical entropy from that generated data, both manually and with the `entropy` package."
      ],
      "metadata": {
        "id": "TrV9l8W7ijB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"entropy\")\n",
        "library(entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAw1GgxolpDm",
        "outputId": "0de01501-30e8-4d80-9950-b0738355ba43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7E2\n",
        "***Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?***"
      ],
      "metadata": {
        "id": "zIYruu3Njrh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p <- c(0.7, 1-0.7)\n",
        "maxP <- c(0.5, 1-0.5)\n",
        "\n",
        "(H1 <- -sum(p*log(p)))\n",
        "(H2 <- sum(p*log(1/p)))\n",
        "\n",
        "(maxH1 <- -sum(maxP*log(maxP)))\n",
        "(maxH2 <- sum(maxP*log(1/maxP)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "rCIxw6cbkSHG",
        "outputId": "4c30b563-f8dd-4a84-f531-d16529c48a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.610864302054894"
            ],
            "text/markdown": "0.610864302054894",
            "text/latex": "0.610864302054894",
            "text/plain": [
              "[1] 0.6108643"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.610864302054894"
            ],
            "text/markdown": "0.610864302054894",
            "text/latex": "0.610864302054894",
            "text/plain": [
              "[1] 0.6108643"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.693147180559945"
            ],
            "text/markdown": "0.693147180559945",
            "text/latex": "0.693147180559945",
            "text/plain": [
              "[1] 0.6931472"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.693147180559945"
            ],
            "text/markdown": "0.693147180559945",
            "text/latex": "0.693147180559945",
            "text/plain": [
              "[1] 0.6931472"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes <- c(\"heads\", \"tails\")\n",
        "d <- sample(x=outcomes, size=100, replace=TRUE, prob=p)\n",
        "\n",
        "(empiricalP <- c(sum(d==\"heads\")/length(d), sum(d==\"tails\")/length(d)))\n",
        "\n",
        "(empiricalH <- -sum(empiricalP*log(empiricalP)))\n",
        "entropy(d) # entropy package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "T7IM6BmDmSGF",
        "outputId": "89c81984-af2d-4d4c-fe7e-1c3dd8eeb063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>0.62</li><li>0.38</li></ol>\n"
            ],
            "text/markdown": "1. 0.62\n2. 0.38\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 0.62\n\\item 0.38\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 0.62 0.38"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.664064126564108"
            ],
            "text/markdown": "0.664064126564108",
            "text/latex": "0.664064126564108",
            "text/plain": [
              "[1] 0.6640641"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.664064126564108"
            ],
            "text/markdown": "0.664064126564108",
            "text/latex": "0.664064126564108",
            "text/plain": [
              "[1] 0.6640641"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7E3\n",
        "***Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?***"
      ],
      "metadata": {
        "id": "QoiYvV2Bpc3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p <- c(0.2, 0.25, 0.25, 0.3)\n",
        "maxP <- c(0.25, 0.25, 0.25, 0.25)\n",
        "\n",
        "(H1 <- -sum(p*log(p)))\n",
        "(H2 <- sum(p*log(1/p)))\n",
        "\n",
        "(maxH1 <- -sum(maxP*log(maxP)))\n",
        "(maxH2 <- sum(maxP*log(1/maxP)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "G2vVVYXnpjpe",
        "outputId": "2b180e30-6763-4782-9016-4d9de87affce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.37622660434455"
            ],
            "text/markdown": "1.37622660434455",
            "text/latex": "1.37622660434455",
            "text/plain": [
              "[1] 1.376227"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.37622660434455"
            ],
            "text/markdown": "1.37622660434455",
            "text/latex": "1.37622660434455",
            "text/plain": [
              "[1] 1.376227"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes <- c(\"1\", \"2\", \"3\", \"4\")\n",
        "d <- sample(x=outcomes, size=100, replace=TRUE, prob=p)\n",
        "\n",
        "(empiricalP <- table(d)/length(d))\n",
        "\n",
        "(empiricalH <- -sum(empiricalP*log(empiricalP)))\n",
        "entropy(d) # entropy package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "yx2rNU-hp0MK",
        "outputId": "127047b2-5189-4c70-fc94-e497466a8a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "d\n",
              "   1    2    3    4 \n",
              "0.23 0.26 0.23 0.28 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38272048392604"
            ],
            "text/markdown": "1.38272048392604",
            "text/latex": "1.38272048392604",
            "text/plain": [
              "[1] 1.38272"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38272048392604"
            ],
            "text/markdown": "1.38272048392604",
            "text/latex": "1.38272048392604",
            "text/plain": [
              "[1] 1.38272"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7E5\n",
        "***Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?***"
      ],
      "metadata": {
        "id": "hbk6FZHeqLjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p <- c(1/3, 1/3, 1/3)\n",
        "maxP <- c(0.25, 0.25, 0.25, 0.25)\n",
        "\n",
        "(H1 <- -sum(p*log(p)))\n",
        "(H2 <- sum(p*log(1/p)))\n",
        "\n",
        "(maxH1 <- -sum(maxP*log(maxP)))\n",
        "(maxH2 <- sum(maxP*log(1/maxP)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f0c7b89c-0dd7-4c28-eb9f-c26747dae00d",
        "id": "qwsc_ozWqWwy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09861228866811"
            ],
            "text/markdown": "1.09861228866811",
            "text/latex": "1.09861228866811",
            "text/plain": [
              "[1] 1.098612"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09861228866811"
            ],
            "text/markdown": "1.09861228866811",
            "text/latex": "1.09861228866811",
            "text/plain": [
              "[1] 1.098612"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes <- c(\"1\", \"2\", \"3\")\n",
        "d <- sample(x=outcomes, size=100, replace=TRUE, prob=p)\n",
        "\n",
        "(empiricalP <- table(d)/length(d))\n",
        "\n",
        "(empiricalH <- -sum(empiricalP*log(empiricalP)))\n",
        "entropy(d) # entropy package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b04163a1-4d87-40cf-a1bb-697cedcc0e40",
        "id": "shDrT-2AqWw6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "d\n",
              "   1    2    3 \n",
              "0.31 0.33 0.36 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09671983946956"
            ],
            "text/markdown": "1.09671983946956",
            "text/latex": "1.09671983946956",
            "text/plain": [
              "[1] 1.09672"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09671983946956"
            ],
            "text/markdown": "1.09671983946956",
            "text/latex": "1.09671983946956",
            "text/plain": [
              "[1] 1.09672"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medium"
      ],
      "metadata": {
        "id": "TI-MQfNNqG8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7M1\n",
        "Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?\n",
        "\n",
        "#### Answer\n",
        "Let's start with WAIC.\n",
        "\n",
        "$$WAIC = -2(lppd_{WAIC} - P_{WAIC})$$\n",
        "\n",
        "where\n",
        "\n",
        "$$lppd_{WAIC} = \\sum_{i=1}^n log(E_\\theta[Pr(y_i | \\theta)])\n",
        "= \\sum_{i=1}^nlog(\\frac{1}{S} \\sum_s Pr(y_i | \\theta_s))$$\n",
        "\n",
        "is the ***log-pointwise-predictive-density*** for WAIC (with uppercase $S$ is the number of posterior draws, lowercase $s$ representing a single posterior draw, and $\\theta$ is the estimated posterior parameters from the draw of $s$) and\n",
        "\n",
        "$$P_{WAIC} = \\sum_{i=1}^n Var_\\theta[log(Pr(y_i | \\theta)]$$\n",
        "\n",
        "is the ***WAIC penalty term***, which takes the variance of each observation $y_i$ across all parameter estimates $\\theta$ from the posterior distribution, then sums them.  The penalty term will tend to be larger for models that are overfit since probability of any observation $y_i$ will change more across the different parameter estimates in the posterior.\n",
        "\n",
        "Now onto AIC (NOTE: McElreath uses a Bayesian AIC, which uses the posterior distribution.  This is unusual; most discussionss of AIC use a frequentist definition, using the maximum likelihood instead of the posterior).  The formula for AIC has a similar structure to that of WAIC:\n",
        "\n",
        "$$AIC = -2(lppd_{AIC} - P_{AIC})$$\n",
        "\n",
        "where\n",
        "\n",
        "$$lppd_{AIC} = \\sum_{i=1}^n log(Pr(y_i | \\theta_{MAP}))$$\n",
        "\n",
        "is the ***log-pointwise-predictive-density*** for AIC of the data, taken at the point value of $\\theta$ that maximizes the posterior, and\n",
        "\n",
        "$$P_{AIC} = \\#\\theta$$\n",
        "\n",
        "is the ***AIC penalty term***, which is just the number of parameters in the model.  The intuition here is that models with more parameters are more likely to overfit and should therefore incur a larger penalty.\n",
        "\n",
        "So the things that distinguish WAIC and AIC are:\n",
        "*   WAIC considers the entire posterior distribution; AIC considers only a single point, the value of $theta$ that maximizes the posterior.\n",
        "*   WAIC estimates a penalty based on variance of fit; AIC uses a asymptotic heuristic to assess overfitting.\n",
        "\n",
        "Thus, WAIC is more general that AIC.  \n",
        "\n",
        "The WAIC and AIC are similar with very large sample sizes.  In this case, maximum value of the posterior (MAP) is a good approximation for the posterior distribution because the spread of the posterior is minimized such that posterior piles tightly around the MAP.\n",
        "\n",
        "But that the posterior converges to the MAP is not exactly the reason that AIC and WAIC converge.  Instead, consider both as $n \\rightarrow \\infty$.\n",
        "\n",
        "$$WAIC_{n \\rightarrow \\infty}\n",
        "= -2(\\sum_{i=1}^{n=\\infty} log(E_\\theta[Pr(y_i | \\theta)]) - \\sum_{i=1}^{n=\\infty} Var_\\theta[log(Pr(y_i | \\theta)])\n",
        "= -2(\\infty - 0) = -\\infty$$\n",
        "\n",
        "$$AIC_{n \\rightarrow \\infty}\n",
        "= -2(\\sum_{i=1}^n log(Pr(y_i | \\theta_{MAP})) - \\# \\theta)\n",
        "= -2(\\infty - \\# \\theta) = -\\infty$$\n",
        "\n",
        "NOTE: As I mentioned above, McElreath uses a slightly unusual definition of AIC.  Usually, the AIC uses $\\theta_{MLE}$, which is the value of $\\theta$ that maximizes the likelihood, rather than $\\theta_{MAP}$.  But the take-home message is the same.  As $n \\rightarrow \\infty$, AIC and WAIC merge.  To see this in the MLE case, notice that the MLE and MAP converge as $n \\rightarrow \\infty$; they also tend to converge if a flat prior is used for the posterior estimates.  Then, the argument follows just the same."
      ],
      "metadata": {
        "id": "Lgo6wNS7qKC6"
      }
    }
  ]
}