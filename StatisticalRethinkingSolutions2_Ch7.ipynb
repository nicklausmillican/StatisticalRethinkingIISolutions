{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOauMRTMDApsjlDxd9ym1U7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicklausmillican/StatisticalRethinkingIISolutions/blob/main/StatisticalRethinkingSolutions2_Ch7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7"
      ],
      "metadata": {
        "id": "TX44yv15HT9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1JPYvU_ERsQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d7ca17-d165-43b3-f84f-86cfed42730f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘httpuv’, ‘xtable’, ‘sourcetools’, ‘later’, ‘promises’, ‘numDeriv’, ‘shiny’, ‘downlit’, ‘htmlwidgets’, ‘abind’, ‘tensorA’, ‘distributional’, ‘Rcpp’, ‘miniUI’, ‘pkgdown’, ‘profvis’, ‘urlchecker’, ‘checkmate’, ‘matrixStats’, ‘posterior’, ‘V8’\n",
            "\n",
            "\n",
            "Downloading GitHub repo rmcelreath/rethinking@slim\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (NA -> 1.4.6.1) [CRAN]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing 1 packages: shape\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m──\u001b[39m \u001b[36mR CMD build\u001b[39m \u001b[36m─────────────────────────────────────────────────────────────────\u001b[39m\n",
            "* checking for file ‘/tmp/RtmpU0Yxnl/remotes26b5f842971/rmcelreath-rethinking-cbcb8ba/DESCRIPTION’ ... OK\n",
            "* preparing ‘rethinking’:\n",
            "* checking DESCRIPTION meta-information ... OK\n",
            "* checking for LF line-endings in source and make files and shell scripts\n",
            "* checking for empty or unneeded directories\n",
            "* looking to see if a ‘data/datalist’ file should be added\n",
            "* building ‘rethinking_2.13.2.tar.gz’\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\"))\n",
        "devtools::install_github(\"rmcelreath/rethinking@slim\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(rethinking)"
      ],
      "metadata": {
        "id": "FFONo5eeR7ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1808467d-cbb4-44a8-e82f-65a5c9e500dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: parallel\n",
            "\n",
            "rethinking (Version 2.13.2)\n",
            "\n",
            "\n",
            "Attaching package: ‘rethinking’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:stats’:\n",
            "\n",
            "    rstudent\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Easy"
      ],
      "metadata": {
        "id": "pJvvqpdCSIOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7E1\n",
        "#### Question\n",
        "State the three motivating criteria that define information entropy. Try to express each in your  own words.\n",
        "\n",
        "#### Answer\n",
        "Entropy is a measure of uncertainty; information resolves that uncertainty (e.g., with evidence).  So entropy *feels* a bit like probability in that both deal with levels of (un)certainty.  It shouldn't be surprising, then, that entropy H is a function of probability.\n",
        "\n",
        "$$H(X) = \\sum_{i=1}^n p(X=x_i) \\times log_b(\\frac{1}{p(X=x_i)})$$\n",
        "$$= - \\sum_{i=1}^n p(X=x_i) \\times log_b(p(X=x_i))$$\n",
        "$$= -E[log_b(p(X=x_i))]$$\n",
        "\n",
        "How to interpret?  Think of $log_b(\\frac{1}{p(X=x_i)})$ as \"surprise\": the greater the probability $p(X=x_i)$, the less surprising it is when that event occurs; conversely, the less probable an event $p(X=x_i)$, the more surprising it is when it occurs.  Next, we *weight* the probability of each event $p(X=x_i)$ by its probability of occuring $p(X=x_i)$ and sum over each event $X=x_i$.  This gives us a *weighted average* of surprise for $X$.  We actually use the $log$ of $\\frac{1}{p(X=x_i)}$ in order to achieve a few desirable traits for our measure of uncertainty.\n",
        "\n",
        "From p. 205:\n",
        "1.   **Continuity:** Just as we want our (un)certainty to be able to slide smoothly from completely uncertain to completely certain, we want informational entropy to do the same.  Probability accomplishes this and, as a function of probability, the formula for $H$ permits the same.\n",
        "2.   **Proportionality:** All else being equal, more potential outcomes for $X$ should increase our uncertainty about $X$.  You can see how the formula for $H$ accomplishes this.  If instead of being, say, 3 values for $X$ there are 4, $H$ becomes the sum of 4 terms instead of only 3; plus, the additional 4th value for $X$ means that the probability of each possible value has probably decreased--thus increasing the surprise for each.  All told, $H$ increases.\n",
        "3.   **Additivity:** If we have two events about which we are uncertain, it is desirable to say that our total uncertainty is the sum of those events.  Similarly for more events.  The formula for $H$ accomplishes this by the $\\sum$ operator.  We sum the surprise, weighted by its probability of happening, of each possible event.\n",
        "\n",
        "Additionally, there are some other important features of information entropy no mentioned in the book:\n",
        "4.   **Non-Negativity:** We cannot be negatively uncertain about an event; we can only be completely certain--which is 0 uncertainty.  The formula for $H$ accomplishes this by its use of probabilities: probabilities also cannot be negative.\n",
        "5.   **Maximal Value:** Just as there is a minimal entropy (0), there is a maximal entropy.  Maximal entropy occurs when each possible value are equally likely.  You can see this in the formula for $H$: if the probability of any event becomes greater than $\\frac{1}{n}$, then the probabilities of other events must decrease...increasing their surprise.  This results in less overall entropy.\n",
        "\n",
        "> This chapter addresses a concept called \"MaxEnt\" or \"maximal entropy\".  This is a similar concept, but it refers to the maximal entropy of a variable under some set of contraints.\n",
        "\n",
        "There are many other features of informational entropy that we could list.  But we'll stop here.\n",
        "\n",
        "If you're interested in a deeper but still-understandable resource on information theory, I suggest [Probability and Information: An Integrated Approach 2nd Edition](https://www.amazon.com/Probability-Information-Integrated-David-Applebaum/dp/0521899044) and [Information Theory: A Tutorial Introduction (2nd Edition)](https://www.amazon.com/Information-Theory-Tutorial-Introduction-2nd/dp/1739672704/ref=sr_1_1?crid=QB4WC07L2QV2&dib=eyJ2IjoiMSJ9.HdyNIMnteFZLf7Ghuh6b4KpfMMBis3Cg2Cn4pOhcL08uhNjOVjY5qqMtASytMwiCDZNo8atQ_BvoUOLSeLvzJMSkRrLHGJNdYa3VnLzWgodcgfMRGbJkHt5VFKslyyzX4JYNra34ExCHrvPo7sXCCkIN3NFpJom82G6K_FzCkaU-mOKz-PkQ3CnNhjkBzmYSdIkVdNUmWSE-Di1EjxG_4rWfCra_68Z8zvOI4yM1ub0uKR_QGV0xFv66L61PHlPS25GTH9hCAOTv5q0nezWOtHUX9fO6YEyj17fuskNrV5M.dpOwik_DNtNOYUJyMa8EC825qBdeL0uQJYgazlfU7l0&dib_tag=se&keywords=information+theory&qid=1709823456&s=books&sprefix=information+theory%2Cstripbooks%2C151&sr=1-1-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)."
      ],
      "metadata": {
        "id": "nm9yt5lBSKk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7E2-7E4\n",
        "#### Questions\n",
        "2.   Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?\n",
        "3.   Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2”  25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?  \n",
        "4.   Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?\n",
        "\n",
        "#### Answers\n",
        "We'll group these answers, since they all follow the same process.  We just need to define the probabilities given in the question, then implement our formula for entropy.\n",
        "\n",
        "For each, I'll calculate entropy in a few different ways, and also compare it to the maximal entropy for that situation described in the question.\n",
        "\n",
        "Finally, I simulate data according to the situation described in the problem, and then calculate the empirical entropy from that generated data, both manually and with the `entropy` package."
      ],
      "metadata": {
        "id": "TrV9l8W7ijB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"entropy\")\n",
        "library(entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAw1GgxolpDm",
        "outputId": "0de01501-30e8-4d80-9950-b0738355ba43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7E2\n",
        "***Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?***"
      ],
      "metadata": {
        "id": "zIYruu3Njrh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p <- c(0.7, 1-0.7)\n",
        "maxP <- c(0.5, 1-0.5)\n",
        "\n",
        "(H1 <- -sum(p*log(p)))\n",
        "(H2 <- sum(p*log(1/p)))\n",
        "\n",
        "(maxH1 <- -sum(maxP*log(maxP)))\n",
        "(maxH2 <- sum(maxP*log(1/maxP)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "rCIxw6cbkSHG",
        "outputId": "4c30b563-f8dd-4a84-f531-d16529c48a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.610864302054894"
            ],
            "text/markdown": "0.610864302054894",
            "text/latex": "0.610864302054894",
            "text/plain": [
              "[1] 0.6108643"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.610864302054894"
            ],
            "text/markdown": "0.610864302054894",
            "text/latex": "0.610864302054894",
            "text/plain": [
              "[1] 0.6108643"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.693147180559945"
            ],
            "text/markdown": "0.693147180559945",
            "text/latex": "0.693147180559945",
            "text/plain": [
              "[1] 0.6931472"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.693147180559945"
            ],
            "text/markdown": "0.693147180559945",
            "text/latex": "0.693147180559945",
            "text/plain": [
              "[1] 0.6931472"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes <- c(\"heads\", \"tails\")\n",
        "d <- sample(x=outcomes, size=100, replace=TRUE, prob=p)\n",
        "\n",
        "(empiricalP <- c(sum(d==\"heads\")/length(d), sum(d==\"tails\")/length(d)))\n",
        "\n",
        "(empiricalH <- -sum(empiricalP*log(empiricalP)))\n",
        "entropy(d) # entropy package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "T7IM6BmDmSGF",
        "outputId": "89c81984-af2d-4d4c-fe7e-1c3dd8eeb063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>0.62</li><li>0.38</li></ol>\n"
            ],
            "text/markdown": "1. 0.62\n2. 0.38\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 0.62\n\\item 0.38\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 0.62 0.38"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.664064126564108"
            ],
            "text/markdown": "0.664064126564108",
            "text/latex": "0.664064126564108",
            "text/plain": [
              "[1] 0.6640641"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.664064126564108"
            ],
            "text/markdown": "0.664064126564108",
            "text/latex": "0.664064126564108",
            "text/plain": [
              "[1] 0.6640641"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7E3\n",
        "***Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?***"
      ],
      "metadata": {
        "id": "QoiYvV2Bpc3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p <- c(0.2, 0.25, 0.25, 0.3)\n",
        "maxP <- c(0.25, 0.25, 0.25, 0.25)\n",
        "\n",
        "(H1 <- -sum(p*log(p)))\n",
        "(H2 <- sum(p*log(1/p)))\n",
        "\n",
        "(maxH1 <- -sum(maxP*log(maxP)))\n",
        "(maxH2 <- sum(maxP*log(1/maxP)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "G2vVVYXnpjpe",
        "outputId": "2b180e30-6763-4782-9016-4d9de87affce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.37622660434455"
            ],
            "text/markdown": "1.37622660434455",
            "text/latex": "1.37622660434455",
            "text/plain": [
              "[1] 1.376227"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.37622660434455"
            ],
            "text/markdown": "1.37622660434455",
            "text/latex": "1.37622660434455",
            "text/plain": [
              "[1] 1.376227"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes <- c(\"1\", \"2\", \"3\", \"4\")\n",
        "d <- sample(x=outcomes, size=100, replace=TRUE, prob=p)\n",
        "\n",
        "(empiricalP <- table(d)/length(d))\n",
        "\n",
        "(empiricalH <- -sum(empiricalP*log(empiricalP)))\n",
        "entropy(d) # entropy package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "yx2rNU-hp0MK",
        "outputId": "127047b2-5189-4c70-fc94-e497466a8a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "d\n",
              "   1    2    3    4 \n",
              "0.23 0.26 0.23 0.28 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38272048392604"
            ],
            "text/markdown": "1.38272048392604",
            "text/latex": "1.38272048392604",
            "text/plain": [
              "[1] 1.38272"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38272048392604"
            ],
            "text/markdown": "1.38272048392604",
            "text/latex": "1.38272048392604",
            "text/plain": [
              "[1] 1.38272"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7E5\n",
        "***Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?***"
      ],
      "metadata": {
        "id": "hbk6FZHeqLjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p <- c(1/3, 1/3, 1/3)\n",
        "maxP <- c(0.25, 0.25, 0.25, 0.25)\n",
        "\n",
        "(H1 <- -sum(p*log(p)))\n",
        "(H2 <- sum(p*log(1/p)))\n",
        "\n",
        "(maxH1 <- -sum(maxP*log(maxP)))\n",
        "(maxH2 <- sum(maxP*log(1/maxP)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f0c7b89c-0dd7-4c28-eb9f-c26747dae00d",
        "id": "qwsc_ozWqWwy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09861228866811"
            ],
            "text/markdown": "1.09861228866811",
            "text/latex": "1.09861228866811",
            "text/plain": [
              "[1] 1.098612"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09861228866811"
            ],
            "text/markdown": "1.09861228866811",
            "text/latex": "1.09861228866811",
            "text/plain": [
              "[1] 1.098612"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.38629436111989"
            ],
            "text/markdown": "1.38629436111989",
            "text/latex": "1.38629436111989",
            "text/plain": [
              "[1] 1.386294"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes <- c(\"1\", \"2\", \"3\")\n",
        "d <- sample(x=outcomes, size=100, replace=TRUE, prob=p)\n",
        "\n",
        "(empiricalP <- table(d)/length(d))\n",
        "\n",
        "(empiricalH <- -sum(empiricalP*log(empiricalP)))\n",
        "entropy(d) # entropy package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b04163a1-4d87-40cf-a1bb-697cedcc0e40",
        "id": "shDrT-2AqWw6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "d\n",
              "   1    2    3 \n",
              "0.31 0.33 0.36 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09671983946956"
            ],
            "text/markdown": "1.09671983946956",
            "text/latex": "1.09671983946956",
            "text/plain": [
              "[1] 1.09672"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "1.09671983946956"
            ],
            "text/markdown": "1.09671983946956",
            "text/latex": "1.09671983946956",
            "text/plain": [
              "[1] 1.09672"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medium"
      ],
      "metadata": {
        "id": "TI-MQfNNqG8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7M1\n",
        "Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?\n",
        "\n",
        "#### Answer\n",
        "Let's start with WAIC.\n",
        "\n",
        "$$WAIC = -2(lppd_{WAIC} - P_{WAIC})$$\n",
        "\n",
        "where\n",
        "\n",
        "$$lppd_{WAIC} = \\sum_{i=1}^n log(E_\\theta[Pr(y_i | \\theta)])\n",
        "= \\sum_{i=1}^nlog(\\frac{1}{S} \\sum_s Pr(y_i | \\theta_s))$$\n",
        "\n",
        "is the ***log-pointwise-predictive-density*** for WAIC (with uppercase $S$ is the number of posterior draws, lowercase $s$ representing a single posterior draw, and $\\theta$ is the estimated posterior parameters from the draw of $s$) and\n",
        "\n",
        "$$P_{WAIC} = \\sum_{i=1}^n Var_\\theta[log(Pr(y_i | \\theta)]$$\n",
        "\n",
        "is the ***WAIC penalty term***, which takes the variance of each observation $y_i$ across all parameter estimates $\\theta$ from the posterior distribution, then sums them.  The penalty term will tend to be larger for models that are overfit since probability of any observation $y_i$ will change more across the different parameter estimates in the posterior.\n",
        "\n",
        "Now onto AIC (NOTE: McElreath uses a Bayesian AIC, which uses the posterior distribution.  This is unusual; most discussionss of AIC use a frequentist definition, using the maximum likelihood instead of the posterior).  The formula for AIC has a similar structure to that of WAIC:\n",
        "\n",
        "$$AIC = -2(lppd_{AIC} - P_{AIC})$$\n",
        "\n",
        "where\n",
        "\n",
        "$$lppd_{AIC} = \\sum_{i=1}^n log(Pr(y_i | \\theta_{MAP}))$$\n",
        "\n",
        "is the ***log-pointwise-predictive-density*** for AIC of the data, taken at the point value of $\\theta$ that maximizes the posterior, and\n",
        "\n",
        "$$P_{AIC} = \\#\\theta$$\n",
        "\n",
        "is the ***AIC penalty term***, which is just the number of parameters in the model.  The intuition here is that models with more parameters are more likely to overfit and should therefore incur a larger penalty.\n",
        "\n",
        "So the things that distinguish WAIC and AIC are:\n",
        "*   WAIC considers the entire posterior distribution; AIC considers only a single point, the value of $theta$ that maximizes the posterior.\n",
        "*   WAIC estimates a penalty based on variance of fit; AIC uses a asymptotic heuristic to assess overfitting.\n",
        "\n",
        "Thus, WAIC is more general that AIC.  \n",
        "\n",
        "The WAIC and AIC are similar with very large sample sizes.  In this case, maximum value of the posterior (MAP) is a good approximation for the posterior distribution because the spread of the posterior is minimized such that posterior piles tightly around the MAP.\n",
        "\n",
        "But that the posterior converges to the MAP is not exactly the reason that AIC and WAIC converge.  Instead, consider both as $n \\rightarrow \\infty$.\n",
        "\n",
        "$$WAIC_{n \\rightarrow \\infty}\n",
        "= -2(\\sum_{i=1}^{n=\\infty} log(E_\\theta[Pr(y_i | \\theta)]) - \\sum_{i=1}^{n=\\infty} Var_\\theta[log(Pr(y_i | \\theta)])\n",
        "= -2(\\infty - 0) = -\\infty$$\n",
        "\n",
        "$$AIC_{n \\rightarrow \\infty}\n",
        "= -2(\\sum_{i=1}^n log(Pr(y_i | \\theta_{MAP})) - \\# \\theta)\n",
        "= -2(\\infty - \\# \\theta) = -\\infty$$\n",
        "\n",
        "NOTE: As I mentioned above, McElreath uses a slightly unusual definition of AIC.  Usually, the AIC uses $\\theta_{MLE}$, which is the value of $\\theta$ that maximizes the likelihood, rather than $\\theta_{MAP}$.  But the take-home message is the same.  As $n \\rightarrow \\infty$, AIC and WAIC merge.  To see this in the MLE case, notice that the MLE and MAP converge as $n \\rightarrow \\infty$; they also tend to converge if a flat prior is used for the posterior estimates.  Then, the argument follows just the same."
      ],
      "metadata": {
        "id": "Lgo6wNS7qKC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7M2\n",
        "Explain the difference between model selection and model comparison. What information is lost under model selection?\n",
        "\n",
        "#### Answer\n",
        "Both **model selection** and **model comparison** use methods such as *information criteria* (WAIC), *Pareto-smoothed importance sampling* (PSIS) or *cross validataion* (CV) to rank the models based on their expected performance with new data.  The difference is that model selection...well...selects the best-ranked model to interpret and report; conversely, model comparison retains for use each model.  There is a third option that is only briefly mentioned in this (2nd) edition of the book, but which is expanded upon in the previous (1st) edition: **model ensemble**, where the predictions of the various models are combined in proportion to their performance in WAIC/PSIS/CV.\n",
        "\n",
        "When we are concerned with making predictions, we're forced to use model ensemble or model selection.  After all, WAIC/PSIS/CV are meant to tell us how well our model is likely to <u>predict</u> unobserved cases.\n",
        "\n",
        "But if we're concerned with inference, we should use model comparison.  WAIC/PSIS/CV do NOT tell us about the causal validity of our model; thus, better scores do not mean more valid models.  Model selection and model ensemble can discard important information regarding the uncertainty of our our model selection.  For instance, if two models have similar claims to inferential validity, WAIC/PSIS/CV scores can help us determine which model is more likely overfitting the data and therefore less generalizable.\n",
        "\n",
        "Let's look at an example.  From the book (p.231), we use the `compare()` function get some PSIS outputs from several models.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "The PSIS scores are the left-most column; the models are listed from *best* PSIS score (top) to *worst* PSIS score (bottom).  In model selection, we would only use the top model `m5.1`.  In model ensemble, we would combine the prediction from each model, weigthing those prediction according to their corresponding value in the `weight` column (so `m5.1` and `m5.3` would get all of the weight).  In model comparison, we would report on each model.  By reporting on each model, and through some systematic testing, we could learn which variables may be more relevant to the system under study.  For instance, is models with explanatory variable X routinely score worse than models without X, we might suspect that X may not be highly relevant to our system."
      ],
      "metadata": {
        "id": "QGLI_D58MZ52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7M3\n",
        "When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.\n",
        "\n",
        "#### Answer\n",
        "The reason that models must be tested against the same data is almost too obvious to state: *Information criteria is used to assess the fit of a model to a particular set of data; in this way, we can compare models.  It would not make sense to compare the performance of one model on one set of set data to the performance of another model on a separate set of data*.\n",
        "\n",
        "Regarding the second part of the question, we should not compare the performance of one model on a set of data against the performance of another model on a subset of the same data because information criteria *sums* the deviations of every observation; more observations will tend to translate into larger scores (which implies worse fit)."
      ],
      "metadata": {
        "id": "6_VVyrXlQitP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7M4\n",
        "What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.\n",
        "\n",
        "#### Answer\n",
        "Rember that the \"effective number of parameters\" refers to the penalty term for PSIS or WAIC.  This raises the question \"What does the number of parameters have to do with penalty?\".  \n",
        "\n",
        "The answer is that, all else being equal, models with more parameters are more flexible and thus are more capable of fitting the data.  Such fitting via flexibility can also lead to *overfitting*, which is when the model mistakes some noise in the data for signal; assuming that mirage signal to be present in unseen data (data that arises from the same data-generating process but on which the model has not been trained), the model will have trouble generalizing to unseen data.  Thus, it makes sense to penalize models by their propensity to overfit.\n",
        "\n",
        "We see the straightforward penalization for parameters with AIC, where the penalty term <u>is</u> the number of parameters.  Again, the idea is that more parameters increase the propensity for overfitting, so AIC penalizes the number of paramters.\n",
        "\n",
        "But especially outside of normal linear models, the number of parameters is just a rough stand-in for flexibility.  Especially outside of normal linear regression, and especially in Bayesian statistics, the connection between the number of parameters and the flexibility of a model becomes a little loose.  We would be more satisfied is we penalized flexibility directly.  This is the objective of PSIS/WAIC/CV.  (I think) we continue to call the PSIS/WAIC penalties \"effective numbers of parameters\" for continuity with their predecessor AIC.\n",
        "\n",
        "Understanding this, we can start reasoning our way through the question.  If features of our model that increase its flexibilty are penalized more heavily by PSIS/WAIC, then features of our model that decrease flexibility will incur less penalty.  What does a more-concentrated prior do, increase or decrease flexibility?  Decreases.  Therefore, a more-concentrated prior will decrease the effective number of parameter i.e., penalty.\n",
        "\n",
        "I think this may merit an experiement.\n",
        "\n",
        "Let's start by making some fake data."
      ],
      "metadata": {
        "id": "YBXz3uGPBDr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=25\n",
        "x <- rnorm(n=n, mean=0, sd=1)\n",
        "y <- 2*x + rnorm(n=n, mean=0, sd=1)\n",
        "\n",
        "dlist <- list(x=x, y=y)"
      ],
      "metadata": {
        "id": "N6CgB4nRLl3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we've made `y` a function of `x`.  Now we'll regress `y ~ x` with a less- and more-concentrated prior, then compare the models."
      ],
      "metadata": {
        "id": "8tlT8hz2LxE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_diffusePrior <- quap(\n",
        "  alist(\n",
        "    y ~ dnorm(mu, sigma),\n",
        "      mu <- b_x * x,\n",
        "        b_x ~ dnorm(2,10), # diffuse prior, sd=10\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=dlist\n",
        ")\n",
        "\n",
        "m_concentratedPrior <- quap(\n",
        "  alist(\n",
        "    y ~ dnorm(mu, sigma),\n",
        "      mu <- b_x * x,\n",
        "        b_x ~ dnorm(2,1), # concentrated prior, sd=1\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=dlist\n",
        ")\n",
        "\n",
        "round(precis(m_diffusePrior),2)\n",
        "round(precis(m_concentratedPrior),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Vzs4Spi9Lwks",
        "outputId": "576a18af-8763-4805-c5ca-a77fcee554a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>5.5%</th><th scope=col>94.5%</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>b_x</th><td>1.66</td><td>0.19</td><td>1.36</td><td>1.96</td></tr>\n",
              "\t<tr><th scope=row>sigma</th><td>0.82</td><td>0.11</td><td>0.64</td><td>1.00</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 4\n\n| <!--/--> | mean &lt;dbl&gt; | sd &lt;dbl&gt; | 5.5% &lt;dbl&gt; | 94.5% &lt;dbl&gt; |\n|---|---|---|---|---|\n| b_x | 1.66 | 0.19 | 1.36 | 1.96 |\n| sigma | 0.82 | 0.11 | 0.64 | 1.00 |\n\n",
            "text/latex": "A data.frame: 2 × 4\n\\begin{tabular}{r|llll}\n  & mean & sd & 5.5\\% & 94.5\\%\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tb\\_x & 1.66 & 0.19 & 1.36 & 1.96\\\\\n\tsigma & 0.82 & 0.11 & 0.64 & 1.00\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      mean sd   5.5% 94.5%\n",
              "b_x   1.66 0.19 1.36 1.96 \n",
              "sigma 0.82 0.11 0.64 1.00 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>5.5%</th><th scope=col>94.5%</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>b_x</th><td>1.67</td><td>0.19</td><td>1.37</td><td>1.97</td></tr>\n",
              "\t<tr><th scope=row>sigma</th><td>0.82</td><td>0.11</td><td>0.64</td><td>1.00</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 4\n\n| <!--/--> | mean &lt;dbl&gt; | sd &lt;dbl&gt; | 5.5% &lt;dbl&gt; | 94.5% &lt;dbl&gt; |\n|---|---|---|---|---|\n| b_x | 1.67 | 0.19 | 1.37 | 1.97 |\n| sigma | 0.82 | 0.11 | 0.64 | 1.00 |\n\n",
            "text/latex": "A data.frame: 2 × 4\n\\begin{tabular}{r|llll}\n  & mean & sd & 5.5\\% & 94.5\\%\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tb\\_x & 1.67 & 0.19 & 1.37 & 1.97\\\\\n\tsigma & 0.82 & 0.11 & 0.64 & 1.00\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      mean sd   5.5% 94.5%\n",
              "b_x   1.67 0.19 1.37 1.97 \n",
              "sigma 0.82 0.11 0.64 1.00 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So our estimates are very similar between the 2 models.  But which do we expect to be at greater risk of overfitting, the diffuse-prior model or the concentrated-prior model?  We can check this directly with the `compare()` function; I'll compare with both WAIC and PSIS?"
      ],
      "metadata": {
        "id": "rH9U8MhVOSKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(compare(m_diffusePrior, m_concentratedPrior, func=WAIC),2)\n",
        "round(compare(m_diffusePrior, m_concentratedPrior, func=PSIS),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "JZmF9XpQNg48",
        "outputId": "d7fd3d1d-e340-4a88-b95d-2670bf04d697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>WAIC</th><th scope=col>SE</th><th scope=col>dWAIC</th><th scope=col>dSE</th><th scope=col>pWAIC</th><th scope=col>weight</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>m_concentratedPrior</th><td>66.32</td><td>7.29</td><td>0.00</td><td>  NA</td><td>2.0</td><td>0.56</td></tr>\n",
              "\t<tr><th scope=row>m_diffusePrior</th><td>66.79</td><td>7.47</td><td>0.46</td><td>0.23</td><td>2.2</td><td>0.44</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 6\n\n| <!--/--> | WAIC &lt;dbl&gt; | SE &lt;dbl&gt; | dWAIC &lt;dbl&gt; | dSE &lt;dbl&gt; | pWAIC &lt;dbl&gt; | weight &lt;dbl&gt; |\n|---|---|---|---|---|---|---|\n| m_concentratedPrior | 66.32 | 7.29 | 0.00 |   NA | 2.0 | 0.56 |\n| m_diffusePrior | 66.79 | 7.47 | 0.46 | 0.23 | 2.2 | 0.44 |\n\n",
            "text/latex": "A data.frame: 2 × 6\n\\begin{tabular}{r|llllll}\n  & WAIC & SE & dWAIC & dSE & pWAIC & weight\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tm\\_concentratedPrior & 66.32 & 7.29 & 0.00 &   NA & 2.0 & 0.56\\\\\n\tm\\_diffusePrior & 66.79 & 7.47 & 0.46 & 0.23 & 2.2 & 0.44\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                    WAIC  SE   dWAIC dSE  pWAIC weight\n",
              "m_concentratedPrior 66.32 7.29 0.00    NA 2.0   0.56  \n",
              "m_diffusePrior      66.79 7.47 0.46  0.23 2.2   0.44  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\n",
            "\n",
            "Some Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>PSIS</th><th scope=col>SE</th><th scope=col>dPSIS</th><th scope=col>dSE</th><th scope=col>pPSIS</th><th scope=col>weight</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>m_concentratedPrior</th><td>66.91</td><td>7.62</td><td>0.00</td><td>  NA</td><td>2.28</td><td>0.59</td></tr>\n",
              "\t<tr><th scope=row>m_diffusePrior</th><td>67.67</td><td>7.95</td><td>0.76</td><td>0.36</td><td>2.67</td><td>0.41</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 6\n\n| <!--/--> | PSIS &lt;dbl&gt; | SE &lt;dbl&gt; | dPSIS &lt;dbl&gt; | dSE &lt;dbl&gt; | pPSIS &lt;dbl&gt; | weight &lt;dbl&gt; |\n|---|---|---|---|---|---|---|\n| m_concentratedPrior | 66.91 | 7.62 | 0.00 |   NA | 2.28 | 0.59 |\n| m_diffusePrior | 67.67 | 7.95 | 0.76 | 0.36 | 2.67 | 0.41 |\n\n",
            "text/latex": "A data.frame: 2 × 6\n\\begin{tabular}{r|llllll}\n  & PSIS & SE & dPSIS & dSE & pPSIS & weight\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tm\\_concentratedPrior & 66.91 & 7.62 & 0.00 &   NA & 2.28 & 0.59\\\\\n\tm\\_diffusePrior & 67.67 & 7.95 & 0.76 & 0.36 & 2.67 & 0.41\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                    PSIS  SE   dPSIS dSE  pPSIS weight\n",
              "m_concentratedPrior 66.91 7.62 0.00    NA 2.28  0.59  \n",
              "m_diffusePrior      67.67 7.95 0.76  0.36 2.67  0.41  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should run every code block from the start of the question to here; the randomly-generated data will give different results, but over several runs you'll see that the outcomes align with our intuitions.  The diffuse-prior model is more likely to overfit, as indicated the `pWAIC` and `pPSIS` output terms, and thereby generalize less well to new data.  Let's see if this latter conclusion is true.\n",
        "\n",
        "Let's redo this, but with a new more-variable `y2` outcome variable.  If we're correct, the concentrated-prior model should be do a better job of discerning between signal and noise--generating more accurate posterior estimates."
      ],
      "metadata": {
        "id": "o-P3CoCyOhOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=25\n",
        "x <- rnorm(n=n, mean=0, sd=1)\n",
        "y2 <- 2*x + rnorm(n=n, mean=0, sd=10)\n",
        "dlist <- list(x=x, y=y)\n",
        "\n",
        "m_diffusePrior <- quap(\n",
        "  alist(\n",
        "    y2 ~ dnorm(mu, sigma),\n",
        "      mu <- b_x * x,\n",
        "        b_x ~ dnorm(2,10), # diffuse prior, sd=10\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=dlist\n",
        ")\n",
        "\n",
        "m_concentratedPrior <- quap(\n",
        "  alist(\n",
        "    y2 ~ dnorm(mu, sigma),\n",
        "      mu <- b_x * x,\n",
        "        b_x ~ dnorm(2,1), # concentrated prior, sd=1\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=dlist\n",
        ")\n",
        "\n",
        "round(precis(m_diffusePrior),2)\n",
        "round(precis(m_concentratedPrior),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Vg2oFyj6Qh0o",
        "outputId": "57bc0a54-3f3f-4aff-c37c-4737d63e9198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>5.5%</th><th scope=col>94.5%</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>b_x</th><td>2.32</td><td>1.34</td><td>0.19</td><td>4.46</td></tr>\n",
              "\t<tr><th scope=row>sigma</th><td>7.89</td><td>0.92</td><td>6.42</td><td>9.36</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 4\n\n| <!--/--> | mean &lt;dbl&gt; | sd &lt;dbl&gt; | 5.5% &lt;dbl&gt; | 94.5% &lt;dbl&gt; |\n|---|---|---|---|---|\n| b_x | 2.32 | 1.34 | 0.19 | 4.46 |\n| sigma | 7.89 | 0.92 | 6.42 | 9.36 |\n\n",
            "text/latex": "A data.frame: 2 × 4\n\\begin{tabular}{r|llll}\n  & mean & sd & 5.5\\% & 94.5\\%\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tb\\_x & 2.32 & 1.34 & 0.19 & 4.46\\\\\n\tsigma & 7.89 & 0.92 & 6.42 & 9.36\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      mean sd   5.5% 94.5%\n",
              "b_x   2.32 1.34 0.19 4.46 \n",
              "sigma 7.89 0.92 6.42 9.36 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>5.5%</th><th scope=col>94.5%</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>b_x</th><td>2.12</td><td>0.80</td><td>0.83</td><td>3.40</td></tr>\n",
              "\t<tr><th scope=row>sigma</th><td>7.89</td><td>0.92</td><td>6.42</td><td>9.36</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 4\n\n| <!--/--> | mean &lt;dbl&gt; | sd &lt;dbl&gt; | 5.5% &lt;dbl&gt; | 94.5% &lt;dbl&gt; |\n|---|---|---|---|---|\n| b_x | 2.12 | 0.80 | 0.83 | 3.40 |\n| sigma | 7.89 | 0.92 | 6.42 | 9.36 |\n\n",
            "text/latex": "A data.frame: 2 × 4\n\\begin{tabular}{r|llll}\n  & mean & sd & 5.5\\% & 94.5\\%\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tb\\_x & 2.12 & 0.80 & 0.83 & 3.40\\\\\n\tsigma & 7.89 & 0.92 & 6.42 & 9.36\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      mean sd   5.5% 94.5%\n",
              "b_x   2.12 0.80 0.83 3.40 \n",
              "sigma 7.89 0.92 6.42 9.36 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure enough, the more-concentrated prior gets closer to the true value of `b_x` (2) than the less-contrated model.  (Again, probably want to rerun several times).\n",
        "\n",
        "Does this mean that a more-concentrated prior is always best?  Of course not.  In fact, we've kinda cheated here.  We set the prior for `b_x` to center on the actual value.  What if we center the prior somewhere else?"
      ],
      "metadata": {
        "id": "jDbtsCKnQpMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_diffusePrior <- quap(\n",
        "  alist(\n",
        "    y2 ~ dnorm(mu, sigma),\n",
        "      mu <- b_x * x,\n",
        "        b_x ~ dnorm(0,10), # diffuse prior, sd=10\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=dlist\n",
        ")\n",
        "\n",
        "m_concentratedPrior <- quap(\n",
        "  alist(\n",
        "    y2 ~ dnorm(mu, sigma),\n",
        "      mu <- b_x * x,\n",
        "        b_x ~ dnorm(0,1), # concentrated prior, sd=1\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=dlist\n",
        ")\n",
        "\n",
        "round(precis(m_diffusePrior),2)\n",
        "round(precis(m_concentratedPrior),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "2X4J7wxlR_Mw",
        "outputId": "7e25ada8-f9d3-4e5d-fb0e-b88c286eba77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>5.5%</th><th scope=col>94.5%</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>b_x</th><td>2.29</td><td>1.34</td><td>0.15</td><td>4.42</td></tr>\n",
              "\t<tr><th scope=row>sigma</th><td>7.89</td><td>0.92</td><td>6.42</td><td>9.36</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 4\n\n| <!--/--> | mean &lt;dbl&gt; | sd &lt;dbl&gt; | 5.5% &lt;dbl&gt; | 94.5% &lt;dbl&gt; |\n|---|---|---|---|---|\n| b_x | 2.29 | 1.34 | 0.15 | 4.42 |\n| sigma | 7.89 | 0.92 | 6.42 | 9.36 |\n\n",
            "text/latex": "A data.frame: 2 × 4\n\\begin{tabular}{r|llll}\n  & mean & sd & 5.5\\% & 94.5\\%\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tb\\_x & 2.29 & 1.34 & 0.15 & 4.42\\\\\n\tsigma & 7.89 & 0.92 & 6.42 & 9.36\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      mean sd   5.5% 94.5%\n",
              "b_x   2.29 1.34 0.15 4.42 \n",
              "sigma 7.89 0.92 6.42 9.36 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 4</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>5.5%</th><th scope=col>94.5%</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>b_x</th><td>0.81</td><td>0.82</td><td>-0.50</td><td>2.11</td></tr>\n",
              "\t<tr><th scope=row>sigma</th><td>8.02</td><td>0.94</td><td> 6.51</td><td>9.53</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 4\n\n| <!--/--> | mean &lt;dbl&gt; | sd &lt;dbl&gt; | 5.5% &lt;dbl&gt; | 94.5% &lt;dbl&gt; |\n|---|---|---|---|---|\n| b_x | 0.81 | 0.82 | -0.50 | 2.11 |\n| sigma | 8.02 | 0.94 |  6.51 | 9.53 |\n\n",
            "text/latex": "A data.frame: 2 × 4\n\\begin{tabular}{r|llll}\n  & mean & sd & 5.5\\% & 94.5\\%\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tb\\_x & 0.81 & 0.82 & -0.50 & 2.11\\\\\n\tsigma & 8.02 & 0.94 &  6.51 & 9.53\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      mean sd   5.5%  94.5%\n",
              "b_x   0.81 0.82 -0.50 2.11 \n",
              "sigma 8.02 0.94  6.51 9.53 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the diffuse-prior model does a better job of estimation.  This is despite the fact that the concentrated model still has a better WAIC/PSIS score.  (Rerun several times)"
      ],
      "metadata": {
        "id": "muxxVuLPSErk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(compare(m_diffusePrior, m_concentratedPrior, func=WAIC),2)\n",
        "round(compare(m_diffusePrior, m_concentratedPrior, func=PSIS),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4uhH9OBPSgwU",
        "outputId": "ecf68ee9-2658-4b40-cc3a-2f9428c91431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>WAIC</th><th scope=col>SE</th><th scope=col>dWAIC</th><th scope=col>dSE</th><th scope=col>pWAIC</th><th scope=col>weight</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>m_concentratedPrior</th><td>186.97</td><td>11.56</td><td>0.00</td><td> NA</td><td>2.24</td><td>0.6</td></tr>\n",
              "\t<tr><th scope=row>m_diffusePrior</th><td>187.78</td><td>11.53</td><td>0.81</td><td>2.4</td><td>3.06</td><td>0.4</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 6\n\n| <!--/--> | WAIC &lt;dbl&gt; | SE &lt;dbl&gt; | dWAIC &lt;dbl&gt; | dSE &lt;dbl&gt; | pWAIC &lt;dbl&gt; | weight &lt;dbl&gt; |\n|---|---|---|---|---|---|---|\n| m_concentratedPrior | 186.97 | 11.56 | 0.00 |  NA | 2.24 | 0.6 |\n| m_diffusePrior | 187.78 | 11.53 | 0.81 | 2.4 | 3.06 | 0.4 |\n\n",
            "text/latex": "A data.frame: 2 × 6\n\\begin{tabular}{r|llllll}\n  & WAIC & SE & dWAIC & dSE & pWAIC & weight\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tm\\_concentratedPrior & 186.97 & 11.56 & 0.00 &  NA & 2.24 & 0.6\\\\\n\tm\\_diffusePrior & 187.78 & 11.53 & 0.81 & 2.4 & 3.06 & 0.4\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                    WAIC   SE    dWAIC dSE pWAIC weight\n",
              "m_concentratedPrior 186.97 11.56 0.00   NA 2.24  0.6   \n",
              "m_diffusePrior      187.78 11.53 0.81  2.4 3.06  0.4   "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\n",
            "\n",
            "Some Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>PSIS</th><th scope=col>SE</th><th scope=col>dPSIS</th><th scope=col>dSE</th><th scope=col>pPSIS</th><th scope=col>weight</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>m_diffusePrior</th><td>187.58</td><td>11.65</td><td>0.00</td><td>  NA</td><td>2.98</td><td>0.58</td></tr>\n",
              "\t<tr><th scope=row>m_concentratedPrior</th><td>188.20</td><td>12.46</td><td>0.62</td><td>2.65</td><td>2.79</td><td>0.42</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 6\n\n| <!--/--> | PSIS &lt;dbl&gt; | SE &lt;dbl&gt; | dPSIS &lt;dbl&gt; | dSE &lt;dbl&gt; | pPSIS &lt;dbl&gt; | weight &lt;dbl&gt; |\n|---|---|---|---|---|---|---|\n| m_diffusePrior | 187.58 | 11.65 | 0.00 |   NA | 2.98 | 0.58 |\n| m_concentratedPrior | 188.20 | 12.46 | 0.62 | 2.65 | 2.79 | 0.42 |\n\n",
            "text/latex": "A data.frame: 2 × 6\n\\begin{tabular}{r|llllll}\n  & PSIS & SE & dPSIS & dSE & pPSIS & weight\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tm\\_diffusePrior & 187.58 & 11.65 & 0.00 &   NA & 2.98 & 0.58\\\\\n\tm\\_concentratedPrior & 188.20 & 12.46 & 0.62 & 2.65 & 2.79 & 0.42\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                    PSIS   SE    dPSIS dSE  pPSIS weight\n",
              "m_diffusePrior      187.58 11.65 0.00    NA 2.98  0.58  \n",
              "m_concentratedPrior 188.20 12.46 0.62  2.65 2.79  0.42  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7M5-7M6\n",
        "#### 7M5\n",
        "Provide an informal explanation of why informative priors reduce overfitting.\n",
        "\n",
        "#### 7M6\n",
        "Provide an informal explanation of why overly informative priors result in underfitting.\n",
        "\n",
        "##### Answers\n",
        "We saw this in the answer to 7M4.  Overfitting occurs when a model mistakes noise in the data for signal.  Priors essentially tell the model which values to entertain as parameter estimates and how seriously to consider them; broader (less-informed) priors will seriously entertain more possible values, facilitating a broader posterior, while narrower (more-informed) priors will focus on fewer possible estimates, facilitating a narrower posterior.  Broader posteriors, resulting from a broader prior, will necessarily give more probabilty mass to inaccurate parameter values--which is the mistaking of noise for signal and thus overfitting.  The narrower posterior, resulting from a narrower prior, *can* filter out much of this noise by simply failing to consider such values.  But a danger lurks: a narrow prior that has excluded reasonable values will make it difficult for the posterior to give adequate weight to these values--this is underfitting."
      ],
      "metadata": {
        "id": "DpF5xxpFRqJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard"
      ],
      "metadata": {
        "id": "fVVSB3a7YopZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7H1\n",
        "In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corporate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed  below), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in `data(Laffer)`. Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a  straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue?\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "#### Answer\n",
        "Let's first look at the data."
      ],
      "metadata": {
        "id": "kGNejnR0Yp_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data(Laffer)\n",
        "d <- Laffer\n",
        "str(d)\n",
        "head(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "BAHQFfGWZNRe",
        "outputId": "05dcb43b-0292-484c-c7d2-f1647c9f3643"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.frame':\t29 obs. of  2 variables:\n",
            " $ tax_rate   : num  0.07 8.81 12.84 16.24 19.18 ...\n",
            " $ tax_revenue: num  -0.06 2.45 3.58 2.19 2.46 1.95 1.25 1.59 3.37 2.87 ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>tax_rate</th><th scope=col>tax_revenue</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td> 0.07</td><td>-0.06</td></tr>\n",
              "\t<tr><th scope=row>2</th><td> 8.81</td><td> 2.45</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>12.84</td><td> 3.58</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>16.24</td><td> 2.19</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>19.18</td><td> 2.46</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>19.29</td><td> 1.95</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 6 × 2\n\n| <!--/--> | tax_rate &lt;dbl&gt; | tax_revenue &lt;dbl&gt; |\n|---|---|---|\n| 1 |  0.07 | -0.06 |\n| 2 |  8.81 |  2.45 |\n| 3 | 12.84 |  3.58 |\n| 4 | 16.24 |  2.19 |\n| 5 | 19.18 |  2.46 |\n| 6 | 19.29 |  1.95 |\n\n",
            "text/latex": "A data.frame: 6 × 2\n\\begin{tabular}{r|ll}\n  & tax\\_rate & tax\\_revenue\\\\\n  & <dbl> & <dbl>\\\\\n\\hline\n\t1 &  0.07 & -0.06\\\\\n\t2 &  8.81 &  2.45\\\\\n\t3 & 12.84 &  3.58\\\\\n\t4 & 16.24 &  2.19\\\\\n\t5 & 19.18 &  2.46\\\\\n\t6 & 19.29 &  1.95\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  tax_rate tax_revenue\n",
              "1  0.07    -0.06      \n",
              "2  8.81     2.45      \n",
              "3 12.84     3.58      \n",
              "4 16.24     2.19      \n",
              "5 19.18     2.46      \n",
              "6 19.29     1.95      "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to fit a straight-line and curved-line regression; for the curved-line regression, I use a quadratic polynomial to kinda-approximate the drawing.  I don't have a good sense of priors, so I'm going to standardize the variables first."
      ],
      "metadata": {
        "id": "uHi6k_HgZ9Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d$Trate <- standardize(d$tax_rate)\n",
        "d$Trev <- standardize(d$tax_revenue)\n",
        "\n",
        "m_line <- quap(\n",
        "  alist(\n",
        "    Trev ~ dnorm(mu, sigma),\n",
        "      mu <- a + b*Trate,\n",
        "        a ~ dnorm(0,0.2),\n",
        "        b ~ dnorm(0, 0.5),\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=d\n",
        ")\n",
        "\n",
        "m_curved <- quap(\n",
        "  alist(\n",
        "    Trev ~ dnorm(mu, sigma),\n",
        "      mu <- a + b1*Trate + b2*Trate^2,\n",
        "        a ~ dnorm(0,0.2),\n",
        "        b1 ~ dnorm(0, 0.5),\n",
        "        b2 ~ dnorm(0, 0.5),\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=d\n",
        ")"
      ],
      "metadata": {
        "id": "tjLClSKtaDsm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could look at the `precis()` summaries, but I'll just jump to the `compare()`.  I'll use PSIS to compare since we are meant to use PSIS in the next question."
      ],
      "metadata": {
        "id": "b2Wth-YCbvIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(compare(m_line, m_curved, func=PSIS),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "qbWvRH0Ob2Qg",
        "outputId": "a1721961-f845-4f37-f5cf-e59ad07984f2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\n",
            "\n",
            "Some Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 2 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>PSIS</th><th scope=col>SE</th><th scope=col>dPSIS</th><th scope=col>dSE</th><th scope=col>pPSIS</th><th scope=col>weight</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>m_curved</th><td>92.85</td><td>28.65</td><td>0.00</td><td>  NA</td><td>9.00</td><td>0.62</td></tr>\n",
              "\t<tr><th scope=row>m_line</th><td>93.79</td><td>27.15</td><td>0.94</td><td>2.17</td><td>8.48</td><td>0.38</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 2 × 6\n\n| <!--/--> | PSIS &lt;dbl&gt; | SE &lt;dbl&gt; | dPSIS &lt;dbl&gt; | dSE &lt;dbl&gt; | pPSIS &lt;dbl&gt; | weight &lt;dbl&gt; |\n|---|---|---|---|---|---|---|\n| m_curved | 92.85 | 28.65 | 0.00 |   NA | 9.00 | 0.62 |\n| m_line | 93.79 | 27.15 | 0.94 | 2.17 | 8.48 | 0.38 |\n\n",
            "text/latex": "A data.frame: 2 × 6\n\\begin{tabular}{r|llllll}\n  & PSIS & SE & dPSIS & dSE & pPSIS & weight\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tm\\_curved & 92.85 & 28.65 & 0.00 &   NA & 9.00 & 0.62\\\\\n\tm\\_line & 93.79 & 27.15 & 0.94 & 2.17 & 8.48 & 0.38\\\\\n\\end{tabular}\n",
            "text/plain": [
              "         PSIS  SE    dPSIS dSE  pPSIS weight\n",
              "m_curved 92.85 28.65 0.00    NA 9.00  0.62  \n",
              "m_line   93.79 27.15 0.94  2.17 8.48  0.38  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should run that line several times.  You'll see the preferred model change between runs, as well as the `weight`.  I'm actually not sure what is causing that (let me know if you do).  Since these are flipping, I'm not sure what to conclude about the relationship between tax rate and tax revenue.  \n",
        "\n",
        "But I also notice a warning: `Some Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.`.  This is the topic of the next question, so let's continue there."
      ],
      "metadata": {
        "id": "qqMKmvzmdMLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7H2\n",
        "In the Laffer data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem.  Then use robust regression with a Student’s t distribution to revisit the curve fitting problem. How  much does a curved relationship depend upon the outlier point?\n",
        "\n",
        "#### Answer\n",
        "From the image accompanying this problem, we can see the weirdo data point.  Let's use the `PSIS()` function to identify it."
      ],
      "metadata": {
        "id": "cD9mflQyeNLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PSIS(m_curved, pointwise=TRUE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1tFzlZlmejrQ",
        "outputId": "032792ff-9777-4611-f545-534f6059fe65"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 29 × 5</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>PSIS</th><th scope=col>lppd</th><th scope=col>penalty</th><th scope=col>std_err</th><th scope=col>k</th></tr>\n",
              "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td> 2.641808</td><td> -1.3209040</td><td>0.26065187</td><td>31.51811</td><td> 0.43893788</td></tr>\n",
              "\t<tr><td> 1.945329</td><td> -0.9726646</td><td>0.05305265</td><td>31.51811</td><td> 0.45595321</td></tr>\n",
              "\t<tr><td> 2.232092</td><td> -1.1160458</td><td>0.06593994</td><td>31.51811</td><td> 0.50247779</td></tr>\n",
              "\t<tr><td> 1.878679</td><td> -0.9393395</td><td>0.02813117</td><td>31.51811</td><td> 0.20861803</td></tr>\n",
              "\t<tr><td> 1.859680</td><td> -0.9298400</td><td>0.02350710</td><td>31.51811</td><td> 0.14776505</td></tr>\n",
              "\t<tr><td> 2.303437</td><td> -1.1517185</td><td>0.04652220</td><td>31.51811</td><td> 0.14218823</td></tr>\n",
              "\t<tr><td> 3.213770</td><td> -1.6068848</td><td>0.12610161</td><td>31.51811</td><td> 0.47134440</td></tr>\n",
              "\t<tr><td> 3.210313</td><td> -1.6051565</td><td>0.06740916</td><td>31.51811</td><td> 0.52062628</td></tr>\n",
              "\t<tr><td> 1.625913</td><td> -0.8129567</td><td>0.01646632</td><td>31.51811</td><td>-0.10828250</td></tr>\n",
              "\t<tr><td> 1.774467</td><td> -0.8872337</td><td>0.01543469</td><td>31.51811</td><td> 0.10209579</td></tr>\n",
              "\t<tr><td> 4.081434</td><td> -2.0407169</td><td>0.14013214</td><td>31.51811</td><td> 0.32056384</td></tr>\n",
              "\t<tr><td>33.493346</td><td>-16.7466728</td><td>8.73063945</td><td>31.51811</td><td> 2.01200967</td></tr>\n",
              "\t<tr><td> 3.709703</td><td> -1.8548515</td><td>0.08348610</td><td>31.51811</td><td> 0.14735388</td></tr>\n",
              "\t<tr><td> 2.310358</td><td> -1.1551789</td><td>0.02080206</td><td>31.51811</td><td> 0.32755175</td></tr>\n",
              "\t<tr><td> 1.608031</td><td> -0.8040154</td><td>0.01674441</td><td>31.51811</td><td>-0.08497429</td></tr>\n",
              "\t<tr><td> 1.658237</td><td> -0.8291185</td><td>0.01541655</td><td>31.51811</td><td>-0.12676389</td></tr>\n",
              "\t<tr><td> 1.610980</td><td> -0.8054898</td><td>0.01719620</td><td>31.51811</td><td>-0.10955282</td></tr>\n",
              "\t<tr><td> 1.646273</td><td> -0.8231367</td><td>0.01748578</td><td>31.51811</td><td>-0.11701405</td></tr>\n",
              "\t<tr><td> 3.578279</td><td> -1.7891393</td><td>0.13057744</td><td>31.51811</td><td> 0.26707002</td></tr>\n",
              "\t<tr><td> 1.655006</td><td> -0.8275029</td><td>0.01585405</td><td>31.51811</td><td>-0.14341808</td></tr>\n",
              "\t<tr><td> 1.758869</td><td> -0.8794346</td><td>0.01523491</td><td>31.51811</td><td>-0.16457468</td></tr>\n",
              "\t<tr><td> 1.676304</td><td> -0.8381518</td><td>0.01923233</td><td>31.51811</td><td> 0.23091124</td></tr>\n",
              "\t<tr><td> 1.683173</td><td> -0.8415863</td><td>0.01972982</td><td>31.51811</td><td> 0.30170114</td></tr>\n",
              "\t<tr><td> 1.692826</td><td> -0.8464132</td><td>0.01927085</td><td>31.51811</td><td> 0.21469388</td></tr>\n",
              "\t<tr><td> 1.811557</td><td> -0.9057784</td><td>0.02062947</td><td>31.51811</td><td> 0.21703342</td></tr>\n",
              "\t<tr><td> 2.218644</td><td> -1.1093222</td><td>0.03546462</td><td>31.51811</td><td> 0.35613527</td></tr>\n",
              "\t<tr><td> 2.215269</td><td> -1.1076346</td><td>0.04517328</td><td>31.51811</td><td> 0.30162587</td></tr>\n",
              "\t<tr><td> 2.230464</td><td> -1.1152318</td><td>0.05931604</td><td>31.51811</td><td> 0.34787226</td></tr>\n",
              "\t<tr><td> 1.794289</td><td> -0.8971446</td><td>0.02594943</td><td>31.51811</td><td> 0.22563887</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 29 × 5\n\n| PSIS &lt;dbl&gt; | lppd &lt;dbl&gt; | penalty &lt;dbl&gt; | std_err &lt;dbl&gt; | k &lt;dbl&gt; |\n|---|---|---|---|---|\n|  2.641808 |  -1.3209040 | 0.26065187 | 31.51811 |  0.43893788 |\n|  1.945329 |  -0.9726646 | 0.05305265 | 31.51811 |  0.45595321 |\n|  2.232092 |  -1.1160458 | 0.06593994 | 31.51811 |  0.50247779 |\n|  1.878679 |  -0.9393395 | 0.02813117 | 31.51811 |  0.20861803 |\n|  1.859680 |  -0.9298400 | 0.02350710 | 31.51811 |  0.14776505 |\n|  2.303437 |  -1.1517185 | 0.04652220 | 31.51811 |  0.14218823 |\n|  3.213770 |  -1.6068848 | 0.12610161 | 31.51811 |  0.47134440 |\n|  3.210313 |  -1.6051565 | 0.06740916 | 31.51811 |  0.52062628 |\n|  1.625913 |  -0.8129567 | 0.01646632 | 31.51811 | -0.10828250 |\n|  1.774467 |  -0.8872337 | 0.01543469 | 31.51811 |  0.10209579 |\n|  4.081434 |  -2.0407169 | 0.14013214 | 31.51811 |  0.32056384 |\n| 33.493346 | -16.7466728 | 8.73063945 | 31.51811 |  2.01200967 |\n|  3.709703 |  -1.8548515 | 0.08348610 | 31.51811 |  0.14735388 |\n|  2.310358 |  -1.1551789 | 0.02080206 | 31.51811 |  0.32755175 |\n|  1.608031 |  -0.8040154 | 0.01674441 | 31.51811 | -0.08497429 |\n|  1.658237 |  -0.8291185 | 0.01541655 | 31.51811 | -0.12676389 |\n|  1.610980 |  -0.8054898 | 0.01719620 | 31.51811 | -0.10955282 |\n|  1.646273 |  -0.8231367 | 0.01748578 | 31.51811 | -0.11701405 |\n|  3.578279 |  -1.7891393 | 0.13057744 | 31.51811 |  0.26707002 |\n|  1.655006 |  -0.8275029 | 0.01585405 | 31.51811 | -0.14341808 |\n|  1.758869 |  -0.8794346 | 0.01523491 | 31.51811 | -0.16457468 |\n|  1.676304 |  -0.8381518 | 0.01923233 | 31.51811 |  0.23091124 |\n|  1.683173 |  -0.8415863 | 0.01972982 | 31.51811 |  0.30170114 |\n|  1.692826 |  -0.8464132 | 0.01927085 | 31.51811 |  0.21469388 |\n|  1.811557 |  -0.9057784 | 0.02062947 | 31.51811 |  0.21703342 |\n|  2.218644 |  -1.1093222 | 0.03546462 | 31.51811 |  0.35613527 |\n|  2.215269 |  -1.1076346 | 0.04517328 | 31.51811 |  0.30162587 |\n|  2.230464 |  -1.1152318 | 0.05931604 | 31.51811 |  0.34787226 |\n|  1.794289 |  -0.8971446 | 0.02594943 | 31.51811 |  0.22563887 |\n\n",
            "text/latex": "A data.frame: 29 × 5\n\\begin{tabular}{lllll}\n PSIS & lppd & penalty & std\\_err & k\\\\\n <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t  2.641808 &  -1.3209040 & 0.26065187 & 31.51811 &  0.43893788\\\\\n\t  1.945329 &  -0.9726646 & 0.05305265 & 31.51811 &  0.45595321\\\\\n\t  2.232092 &  -1.1160458 & 0.06593994 & 31.51811 &  0.50247779\\\\\n\t  1.878679 &  -0.9393395 & 0.02813117 & 31.51811 &  0.20861803\\\\\n\t  1.859680 &  -0.9298400 & 0.02350710 & 31.51811 &  0.14776505\\\\\n\t  2.303437 &  -1.1517185 & 0.04652220 & 31.51811 &  0.14218823\\\\\n\t  3.213770 &  -1.6068848 & 0.12610161 & 31.51811 &  0.47134440\\\\\n\t  3.210313 &  -1.6051565 & 0.06740916 & 31.51811 &  0.52062628\\\\\n\t  1.625913 &  -0.8129567 & 0.01646632 & 31.51811 & -0.10828250\\\\\n\t  1.774467 &  -0.8872337 & 0.01543469 & 31.51811 &  0.10209579\\\\\n\t  4.081434 &  -2.0407169 & 0.14013214 & 31.51811 &  0.32056384\\\\\n\t 33.493346 & -16.7466728 & 8.73063945 & 31.51811 &  2.01200967\\\\\n\t  3.709703 &  -1.8548515 & 0.08348610 & 31.51811 &  0.14735388\\\\\n\t  2.310358 &  -1.1551789 & 0.02080206 & 31.51811 &  0.32755175\\\\\n\t  1.608031 &  -0.8040154 & 0.01674441 & 31.51811 & -0.08497429\\\\\n\t  1.658237 &  -0.8291185 & 0.01541655 & 31.51811 & -0.12676389\\\\\n\t  1.610980 &  -0.8054898 & 0.01719620 & 31.51811 & -0.10955282\\\\\n\t  1.646273 &  -0.8231367 & 0.01748578 & 31.51811 & -0.11701405\\\\\n\t  3.578279 &  -1.7891393 & 0.13057744 & 31.51811 &  0.26707002\\\\\n\t  1.655006 &  -0.8275029 & 0.01585405 & 31.51811 & -0.14341808\\\\\n\t  1.758869 &  -0.8794346 & 0.01523491 & 31.51811 & -0.16457468\\\\\n\t  1.676304 &  -0.8381518 & 0.01923233 & 31.51811 &  0.23091124\\\\\n\t  1.683173 &  -0.8415863 & 0.01972982 & 31.51811 &  0.30170114\\\\\n\t  1.692826 &  -0.8464132 & 0.01927085 & 31.51811 &  0.21469388\\\\\n\t  1.811557 &  -0.9057784 & 0.02062947 & 31.51811 &  0.21703342\\\\\n\t  2.218644 &  -1.1093222 & 0.03546462 & 31.51811 &  0.35613527\\\\\n\t  2.215269 &  -1.1076346 & 0.04517328 & 31.51811 &  0.30162587\\\\\n\t  2.230464 &  -1.1152318 & 0.05931604 & 31.51811 &  0.34787226\\\\\n\t  1.794289 &  -0.8971446 & 0.02594943 & 31.51811 &  0.22563887\\\\\n\\end{tabular}\n",
            "text/plain": [
              "   PSIS      lppd        penalty    std_err  k          \n",
              "1   2.641808  -1.3209040 0.26065187 31.51811  0.43893788\n",
              "2   1.945329  -0.9726646 0.05305265 31.51811  0.45595321\n",
              "3   2.232092  -1.1160458 0.06593994 31.51811  0.50247779\n",
              "4   1.878679  -0.9393395 0.02813117 31.51811  0.20861803\n",
              "5   1.859680  -0.9298400 0.02350710 31.51811  0.14776505\n",
              "6   2.303437  -1.1517185 0.04652220 31.51811  0.14218823\n",
              "7   3.213770  -1.6068848 0.12610161 31.51811  0.47134440\n",
              "8   3.210313  -1.6051565 0.06740916 31.51811  0.52062628\n",
              "9   1.625913  -0.8129567 0.01646632 31.51811 -0.10828250\n",
              "10  1.774467  -0.8872337 0.01543469 31.51811  0.10209579\n",
              "11  4.081434  -2.0407169 0.14013214 31.51811  0.32056384\n",
              "12 33.493346 -16.7466728 8.73063945 31.51811  2.01200967\n",
              "13  3.709703  -1.8548515 0.08348610 31.51811  0.14735388\n",
              "14  2.310358  -1.1551789 0.02080206 31.51811  0.32755175\n",
              "15  1.608031  -0.8040154 0.01674441 31.51811 -0.08497429\n",
              "16  1.658237  -0.8291185 0.01541655 31.51811 -0.12676389\n",
              "17  1.610980  -0.8054898 0.01719620 31.51811 -0.10955282\n",
              "18  1.646273  -0.8231367 0.01748578 31.51811 -0.11701405\n",
              "19  3.578279  -1.7891393 0.13057744 31.51811  0.26707002\n",
              "20  1.655006  -0.8275029 0.01585405 31.51811 -0.14341808\n",
              "21  1.758869  -0.8794346 0.01523491 31.51811 -0.16457468\n",
              "22  1.676304  -0.8381518 0.01923233 31.51811  0.23091124\n",
              "23  1.683173  -0.8415863 0.01972982 31.51811  0.30170114\n",
              "24  1.692826  -0.8464132 0.01927085 31.51811  0.21469388\n",
              "25  1.811557  -0.9057784 0.02062947 31.51811  0.21703342\n",
              "26  2.218644  -1.1093222 0.03546462 31.51811  0.35613527\n",
              "27  2.215269  -1.1076346 0.04517328 31.51811  0.30162587\n",
              "28  2.230464  -1.1152318 0.05931604 31.51811  0.34787226\n",
              "29  1.794289  -0.8971446 0.02594943 31.51811  0.22563887"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting `pointwise` to `TRUE` breaks down the PSIS values for each observation.\n",
        " The `k` values tell us how influential a point is in determining the fit of the regression; high k values indicate an influential data point; overly influential data points make it more difficult for our model to generalize to unseen data.\n",
        "\n",
        " Here, we see that the 12th data point is highly influential, with `k = 1.92`.  I suppose that *is* our measure of its influence.  We might gain some intuition if we compared the current output with the identical model trained on a truncated data set that omitted observation 12.  But then we couldn't use PSIS or WAIC to compare since they will not have been trained on identical data.  Instead, perhaps we could use leave-one-out cross validation to train the model on several truncated data sets, each time omitting a different observation."
      ],
      "metadata": {
        "id": "IX0KnfcMevOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loo_scores <- data.frame(observation = seq_along(1:nrow(d)),\n",
        "                         loo_score = cv_quap(m_curved, pw=TRUE))\n",
        "\n",
        "loo_scores\n",
        "\n",
        "plot(loo_scores$loo_score, type=\"h\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NmkQSA2bhtQh",
        "outputId": "1e621d55-2508-4955-9bfb-ba30905b0d72"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 29 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>observation</th><th scope=col>loo_score</th></tr>\n",
              "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td> 1</td><td> -1.4870646</td></tr>\n",
              "\t<tr><td> 2</td><td> -0.9868770</td></tr>\n",
              "\t<tr><td> 3</td><td> -1.1270708</td></tr>\n",
              "\t<tr><td> 4</td><td> -0.9205831</td></tr>\n",
              "\t<tr><td> 5</td><td> -0.9190627</td></tr>\n",
              "\t<tr><td> 6</td><td> -1.1352410</td></tr>\n",
              "\t<tr><td> 7</td><td> -1.5528912</td></tr>\n",
              "\t<tr><td> 8</td><td> -1.5834350</td></tr>\n",
              "\t<tr><td> 9</td><td> -0.8037079</td></tr>\n",
              "\t<tr><td>10</td><td> -0.8801019</td></tr>\n",
              "\t<tr><td>11</td><td> -2.0257963</td></tr>\n",
              "\t<tr><td>12</td><td>-14.2080227</td></tr>\n",
              "\t<tr><td>13</td><td> -1.8458754</td></tr>\n",
              "\t<tr><td>14</td><td> -1.1570435</td></tr>\n",
              "\t<tr><td>15</td><td> -0.8023770</td></tr>\n",
              "\t<tr><td>16</td><td> -0.8262448</td></tr>\n",
              "\t<tr><td>17</td><td> -0.7974040</td></tr>\n",
              "\t<tr><td>18</td><td> -0.8144864</td></tr>\n",
              "\t<tr><td>19</td><td> -1.7713659</td></tr>\n",
              "\t<tr><td>20</td><td> -0.8174944</td></tr>\n",
              "\t<tr><td>21</td><td> -0.8759118</td></tr>\n",
              "\t<tr><td>22</td><td> -0.8319702</td></tr>\n",
              "\t<tr><td>23</td><td> -0.8416904</td></tr>\n",
              "\t<tr><td>24</td><td> -0.8503198</td></tr>\n",
              "\t<tr><td>25</td><td> -0.9093343</td></tr>\n",
              "\t<tr><td>26</td><td> -1.1207636</td></tr>\n",
              "\t<tr><td>27</td><td> -1.0969064</td></tr>\n",
              "\t<tr><td>28</td><td> -1.1275310</td></tr>\n",
              "\t<tr><td>29</td><td> -0.9030278</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 29 × 2\n\n| observation &lt;int&gt; | loo_score &lt;dbl&gt; |\n|---|---|\n|  1 |  -1.4870646 |\n|  2 |  -0.9868770 |\n|  3 |  -1.1270708 |\n|  4 |  -0.9205831 |\n|  5 |  -0.9190627 |\n|  6 |  -1.1352410 |\n|  7 |  -1.5528912 |\n|  8 |  -1.5834350 |\n|  9 |  -0.8037079 |\n| 10 |  -0.8801019 |\n| 11 |  -2.0257963 |\n| 12 | -14.2080227 |\n| 13 |  -1.8458754 |\n| 14 |  -1.1570435 |\n| 15 |  -0.8023770 |\n| 16 |  -0.8262448 |\n| 17 |  -0.7974040 |\n| 18 |  -0.8144864 |\n| 19 |  -1.7713659 |\n| 20 |  -0.8174944 |\n| 21 |  -0.8759118 |\n| 22 |  -0.8319702 |\n| 23 |  -0.8416904 |\n| 24 |  -0.8503198 |\n| 25 |  -0.9093343 |\n| 26 |  -1.1207636 |\n| 27 |  -1.0969064 |\n| 28 |  -1.1275310 |\n| 29 |  -0.9030278 |\n\n",
            "text/latex": "A data.frame: 29 × 2\n\\begin{tabular}{ll}\n observation & loo\\_score\\\\\n <int> & <dbl>\\\\\n\\hline\n\t  1 &  -1.4870646\\\\\n\t  2 &  -0.9868770\\\\\n\t  3 &  -1.1270708\\\\\n\t  4 &  -0.9205831\\\\\n\t  5 &  -0.9190627\\\\\n\t  6 &  -1.1352410\\\\\n\t  7 &  -1.5528912\\\\\n\t  8 &  -1.5834350\\\\\n\t  9 &  -0.8037079\\\\\n\t 10 &  -0.8801019\\\\\n\t 11 &  -2.0257963\\\\\n\t 12 & -14.2080227\\\\\n\t 13 &  -1.8458754\\\\\n\t 14 &  -1.1570435\\\\\n\t 15 &  -0.8023770\\\\\n\t 16 &  -0.8262448\\\\\n\t 17 &  -0.7974040\\\\\n\t 18 &  -0.8144864\\\\\n\t 19 &  -1.7713659\\\\\n\t 20 &  -0.8174944\\\\\n\t 21 &  -0.8759118\\\\\n\t 22 &  -0.8319702\\\\\n\t 23 &  -0.8416904\\\\\n\t 24 &  -0.8503198\\\\\n\t 25 &  -0.9093343\\\\\n\t 26 &  -1.1207636\\\\\n\t 27 &  -1.0969064\\\\\n\t 28 &  -1.1275310\\\\\n\t 29 &  -0.9030278\\\\\n\\end{tabular}\n",
            "text/plain": [
              "   observation loo_score  \n",
              "1   1           -1.4870646\n",
              "2   2           -0.9868770\n",
              "3   3           -1.1270708\n",
              "4   4           -0.9205831\n",
              "5   5           -0.9190627\n",
              "6   6           -1.1352410\n",
              "7   7           -1.5528912\n",
              "8   8           -1.5834350\n",
              "9   9           -0.8037079\n",
              "10 10           -0.8801019\n",
              "11 11           -2.0257963\n",
              "12 12          -14.2080227\n",
              "13 13           -1.8458754\n",
              "14 14           -1.1570435\n",
              "15 15           -0.8023770\n",
              "16 16           -0.8262448\n",
              "17 17           -0.7974040\n",
              "18 18           -0.8144864\n",
              "19 19           -1.7713659\n",
              "20 20           -0.8174944\n",
              "21 21           -0.8759118\n",
              "22 22           -0.8319702\n",
              "23 23           -0.8416904\n",
              "24 24           -0.8503198\n",
              "25 25           -0.9093343\n",
              "26 26           -1.1207636\n",
              "27 27           -1.0969064\n",
              "28 28           -1.1275310\n",
              "29 29           -0.9030278"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plot without title"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC+lBMVEUAAAABAQECAgIDAwME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"
          },
          "metadata": {
            "image/png": {
              "width": 420,
              "height": 420
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see the magnitude of the observation 12's influence.\n",
        "\n",
        "Finally, we're asked to try a final *robust regression* model, using the t-distribution."
      ],
      "metadata": {
        "id": "y4SWCA4DiPkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_line_t <- quap(\n",
        "  alist(\n",
        "    Trev ~ dstudent(2, mu, sigma),\n",
        "      mu <- a + b*Trate,\n",
        "        a ~ dnorm(0,0.2),\n",
        "        b ~ dnorm(0, 0.5),\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=d\n",
        ")\n",
        "\n",
        "m_curved_t <- quap(\n",
        "  alist(\n",
        "    Trev ~ dstudent(2, mu, sigma),\n",
        "      mu <- a + b1*Trate + b2*Trate^2,\n",
        "        a ~ dnorm(0,0.2),\n",
        "        b1 ~ dnorm(0, 0.5),\n",
        "        b2 ~ dnorm(0, 0.5),\n",
        "      sigma ~ dexp(1)\n",
        "  ), data=d\n",
        ")\n",
        "\n",
        "round(compare(m_line, m_curved, m_line_t, m_curved_t, func=PSIS),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "tKYOO-4XjTdR",
        "outputId": "33106ca8-496a-4f54-ca24-a13b02b305cd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\n",
            "\n",
            "Some Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 4 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>PSIS</th><th scope=col>SE</th><th scope=col>dPSIS</th><th scope=col>dSE</th><th scope=col>pPSIS</th><th scope=col>weight</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>m_curved_t</th><td>70.60</td><td>14.25</td><td> 0.00</td><td>   NA</td><td>3.83</td><td>0.91</td></tr>\n",
              "\t<tr><th scope=row>m_line_t</th><td>75.12</td><td>13.87</td><td> 4.51</td><td> 4.97</td><td>4.11</td><td>0.09</td></tr>\n",
              "\t<tr><th scope=row>m_curved</th><td>91.05</td><td>27.18</td><td>20.45</td><td>18.39</td><td>8.13</td><td>0.00</td></tr>\n",
              "\t<tr><th scope=row>m_line</th><td>94.16</td><td>27.44</td><td>23.56</td><td>18.96</td><td>8.68</td><td>0.00</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 4 × 6\n\n| <!--/--> | PSIS &lt;dbl&gt; | SE &lt;dbl&gt; | dPSIS &lt;dbl&gt; | dSE &lt;dbl&gt; | pPSIS &lt;dbl&gt; | weight &lt;dbl&gt; |\n|---|---|---|---|---|---|---|\n| m_curved_t | 70.60 | 14.25 |  0.00 |    NA | 3.83 | 0.91 |\n| m_line_t | 75.12 | 13.87 |  4.51 |  4.97 | 4.11 | 0.09 |\n| m_curved | 91.05 | 27.18 | 20.45 | 18.39 | 8.13 | 0.00 |\n| m_line | 94.16 | 27.44 | 23.56 | 18.96 | 8.68 | 0.00 |\n\n",
            "text/latex": "A data.frame: 4 × 6\n\\begin{tabular}{r|llllll}\n  & PSIS & SE & dPSIS & dSE & pPSIS & weight\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\tm\\_curved\\_t & 70.60 & 14.25 &  0.00 &    NA & 3.83 & 0.91\\\\\n\tm\\_line\\_t & 75.12 & 13.87 &  4.51 &  4.97 & 4.11 & 0.09\\\\\n\tm\\_curved & 91.05 & 27.18 & 20.45 & 18.39 & 8.13 & 0.00\\\\\n\tm\\_line & 94.16 & 27.44 & 23.56 & 18.96 & 8.68 & 0.00\\\\\n\\end{tabular}\n",
            "text/plain": [
              "           PSIS  SE    dPSIS dSE   pPSIS weight\n",
              "m_curved_t 70.60 14.25  0.00    NA 3.83  0.91  \n",
              "m_line_t   75.12 13.87  4.51  4.97 4.11  0.09  \n",
              "m_curved   91.05 27.18 20.45 18.39 8.13  0.00  \n",
              "m_line     94.16 27.44 23.56 18.96 8.68  0.00  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The robust regression perform better with `PSIS()`, and the curved-t better than the line-t."
      ],
      "metadata": {
        "id": "jP4b5I67kCQr"
      }
    }
  ]
}